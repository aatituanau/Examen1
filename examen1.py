# -*- coding: utf-8 -*-
"""Examen1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16N5uCD1T-7tCtRSAEDKs_2XCNsq89p5W
"""

## Load dependencies
import requests
import pandas as pd
from bs4 import BeautifulSoup # permite leer y analizar el contenido de páginas web.

#1. Escrapear la tabla Wikipedia

def tablas_scrapy(html, table_selector=None):
    """
    Extrae todas las tablas de un HTML, manejando inconsistencias en headers.
    """
    soup = html if isinstance(html, BeautifulSoup) else BeautifulSoup(html, 'html.parser')  # Verifica y convierte en objeto beautifulsoup para buscar cosas dentro
    tables = soup.select(table_selector) if table_selector else soup.find_all('table')  # Busca todas las tablas

    dfs = []

    # Lo siguiente recorrera el contenido de cada tabla, cada fila <tr> y cada celda <th> y guardara su contenido
    for table in tables:
        rows = []
        for tr in table.find_all('tr'):
            row = []
            for td in tr.find_all(['th', 'td']):
                text = td.get_text(' ', strip=True)
                row.append(text if text else None)
            if row:
                rows.append(row)


        # Determinar headers solo si coinciden con el número de columnas de los datos
        headers = None
        if rows:
            first_row = rows[0]
            # Mejor verificación de fila de encabezado
            th_in_first_row = any(tag.name == 'th' for tag in table.find('tr').find_all(['th', 'td']))
            if th_in_first_row and len(rows) > 1 and len(first_row) == len(rows[1]):
                headers = [str(h).strip() for h in first_row]  # Limpieza adicional de headers
                data = rows[1:]
            else:
                data = rows

        # Asegurar que todas las filas tengan el mismo número de columnas, rellena con none las vacias
        if data:  # Solo si hay datos
            max_cols = max(len(row) for row in data)
            data_normalized = []
            for row in data:
                if len(row) < max_cols:
                    row += [None] * (max_cols - len(row))
                data_normalized.append(row)

            # Crear DataFrame (usar headers solo si coinciden)
            if headers and len(headers) == max_cols:
                df = pd.DataFrame(data_normalized, columns=headers)
            else:
                df = pd.DataFrame(data_normalized)

            # Mejor manejo de columnas duplicadas, les asigna un numero a cada columna duplicada
            if len(df.columns) != len(set(df.columns)):
                df.columns = [f'{col}_{i}' if list(df.columns).count(col) > 1 else col
                             for i, col in enumerate(df.columns)]

            dfs.append(df)

    return dfs

# Obtener el HTML
url = 'https://en.wikipedia.org/wiki/List_of_Latin_phrases_(full)#C'
r = requests.get(url)
dfs = tablas_scrapy(r.text)

# Mostrar número de tablas encontradas
num_tablas = len(dfs)
print(f"\nSe encontraron {num_tablas} tablas en la página.")

#Agrupar todas las tablas encontradas
df = pd.concat(dfs, ignore_index=True)
df

#Agrupar el listado de latin e ingles
tabla=df[['Latin', 'Translation']]
tabla

#Guardar toda la tabla generada en un xlsx

tabla.to_excel("tabla_latin_ingles.xlsx", index=False)
print("Tabla guardada como tabla_latin_ingles.xlsx")

#Dar el listado de las palabras que mas se repiten en latin y en ingles sobre las frases
print("Palabras más repetidas en inglés:")
palabras_ingles= tabla['Translation'].str.split().explode().value_counts()
palabras_ingles

palabras_latin= tabla['Latin'].str.split().explode().value_counts()
print("\nPalabras más repetidas en latín:")
palabras_latin

!pip install deep-translator
from deep_translator import GoogleTranslator

# Tomamos las 10 palabras más repetidas en inglés que ya obtuviste
top_palabras_ingles = palabras_ingles.head(10).index.tolist()
print("Palabras más frecuentes en inglés:", top_palabras_ingles)

# Traducimos cada palabra automáticamente al español
palabras_traducidas = []
for palabra in top_palabras_ingles:
    try:
        traduccion = GoogleTranslator(source='auto', target='es').translate(palabra)
        palabras_traducidas.append(traduccion.lower())
    except Exception as e:
        print(f"Error al traducir '{palabra}': {e}")
        palabras_traducidas.append(palabra.lower())

print("\nPalabras traducidas al español:")
print(palabras_traducidas)

#En base a las palabras y verbos que mas se usen dar 5 nuevas frases en español
frases = []

for palabra in palabras_traducidas:
    frase = f"La palabra '{palabra}' es fundamental en la filosofía y la cultura."
    frases.append(frase)

# Mostrar frases
for i, f in enumerate(frases, 1):
    print(f"{i}. {f}")

"""2. Guardar en un xlsx
3. Dar el istado de las palabras que mas se repiten en latin y en ingles sobre las frases
4. En base a las palabras y verbos que mas se usen dar 5 nuevas frases en español
5. Subir a git y enviar el py a la UVirtual
"""